var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = PIQL","category":"page"},{"location":"#PIQL","page":"Home","title":"PIQL","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for PIQL.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [PIQL]","category":"page"},{"location":"#PIQL.ControlProblem","page":"Home","title":"PIQL.ControlProblem","text":"ControlProblem is a struct with fields that are      the functions which completely define a KL-control problem\n\n\n\n\n\n","category":"type"},{"location":"#PIQL.PVActor","page":"Home","title":"PIQL.PVActor","text":"We write this as a separation between policy and value function The policy is representable as a set of unnormalized weights w_i so that \n\nlog(pi) = log(wi) - log(sum(w))  ηi = log(wi/πi) - log(sum(w))  ηi = ωi - log(sum(πi * exp(ω_i)))\n\nThe ωi = log(wi/π_i) have an estimator in\n\nβ(criticq - actorv)i = hatωi => ω_i\n\nThere's no exponential sampling that needs to be averaged so we can simply update (with a pretty strong learning rate.)\n\nωi(t+1) = ωi(t) + α (hatωi -  ωi(t))\n\nat the same time, the actorq can be generated\n\nactorqi = ηi / β + actorvi = (ωi - log(Ξ)) / β + actorv\n\nand it can be constructed from \n\nactorqi = ηi / β + actorvi = (qi - log(Ξ)) / β + actorv\n\nv = sum(πi exp(β*actorqi)) / β w = β * (actorq_i - v)\n\nStill experimental, loss interest after TabActor update rule didn't work.\n\n\n\n\n\n","category":"type"},{"location":"#PIQL.PiqlParticle","page":"Home","title":"PIQL.PiqlParticle","text":"current StateAction definition:\n\nstruct StateAction{S,A} # static and constructed on forward pass     # atomic unit of data for all reinforcement learning     # includes all information and diagnostics available from a single step.     state::S     action::A     β::Float64 # the beta under which the temperature is allowed to fluxuate     actorq::Float64     criticq::Float64     reward::Float64 # actually incurred cost entering the state     V::Float64 # free energy of current action (observed value)     U::Float64 # average energy of current action end\n\n\n\n\n\n","category":"type"},{"location":"#PIQL.empty_actor-Tuple{}","page":"Home","title":"PIQL.empty_actor","text":"We define an actor interface: an actor is a callable struct\n\nstruct GenericActor     parameters\n\nend\n\nfunction (a::Actor)(state,action)     ...     return E_actor end\n\nfunction loss(actor, minibatch)     ...     return E_actor end\n\n\n\n\n\n","category":"method"},{"location":"#PIQL.initial_state_action-Tuple{ControlProblem, Any}","page":"Home","title":"PIQL.initial_state_action","text":"Start off a trajectory with a new state action pair\n\n\n\n\n\n","category":"method"},{"location":"#PIQL.qestimate-NTuple{4, Any}","page":"Home","title":"PIQL.qestimate","text":"z(t) = exp(β(criticq - actorq)) (γ z(t+1) + (1-γ)) z(t) = exp(β(criticq - actorq)) (γ (z(t+1)-1) + 1) logz(t) = β(criticq - actorq) +  log(γ(z(t+1)-1) + 1) = β(criticq - actorq) +  logp1(γexpm1(logz(t+1)))\n\n\n\n\n\n","category":"method"},{"location":"#PIQL.run_piql!-Tuple{Any, Any, Any}","page":"Home","title":"PIQL.run_piql!","text":"Expected dynamics: grow the worldline until the terminal condition. then backpropogate weights and return terminated = true\n\n\n\n\n\n","category":"method"},{"location":"#PIQL.step_choices-Tuple{}","page":"Home","title":"PIQL.step_choices","text":"Actions are indexed by natural numbers\n\n\n\n\n\n","category":"method"},{"location":"#PIQL.train!-Tuple{TabularActor, Any}","page":"Home","title":"PIQL.train!","text":"Destructive memory training, storing partition function memory is composed of struct (at time of writing) consisting of\n\nstruct QEstimate{S,A}\n    sa::StateAction{S,A} # the state action information at this time step\n    criticq::Float64 # from the state one step forward in time\n    logz::Float64 # log z at state in sa.state\nend\n\n\n\n\n\n","category":"method"}]
}
